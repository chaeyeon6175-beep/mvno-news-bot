import os, requests, re, time
from datetime import datetime, timedelta
from difflib import SequenceMatcher

# 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
NAVER_ID = os.environ.get('NAVER_CLIENT_ID')
NAVER_SECRET = os.environ.get('NAVER_CLIENT_SECRET')
NOTION_TOKEN = os.environ.get('NOTION_TOKEN')
DB_IDS = {
    "MNO": os.environ.get('DB_ID_MNO'),
    "SUBSID": os.environ.get('DB_ID_SUBSID'),
    "FIN": os.environ.get('DB_ID_FIN'),
    "SMALL": os.environ.get('DB_ID_SMALL')
}

HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

def clear_database(db_id):
    """ìˆ˜ì§‘ ì „ ê¸°ì¡´ ê¸°ì‚¬ ì‚­ì œ"""
    print(f"ğŸ§¹ ë°ì´í„°ë² ì´ìŠ¤ ë¹„ìš°ê¸°: {db_id}")
    query_url = f"https://api.notion.com/v1/databases/{db_id}/query"
    while True:
        res = requests.post(query_url, headers=HEADERS)
        results = res.json().get("results", [])
        if not results: break
        for page in results:
            requests.patch(f"https://api.notion.com/v1/pages/{page['id']}", headers=HEADERS, json={"archived": True})
        if not res.json().get("has_more"): break

def get_similarity(a, b):
    a = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', a); b = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', b)
    return SequenceMatcher(None, a, b).ratio()

def is_telecom_news(title):
    t = title.lower().replace(' ', '')
    exclude = ["ì•¼êµ¬", "ë°°êµ¬", "ë†êµ¬", "ì¶•êµ¬", "ìŠ¤í¬ì¸ ", "ì‡¼í•‘", "ì£¼ê°€", "ì¦ì‹œ", "ìƒì¥"]
    if any(ex in t for ex in exclude): return False
    include = ["ìš”ê¸ˆì œ", "ì•Œëœ°í°", "mvno", "5g", "6g", "lte", "í†µì‹ ", "ê°€ì…ì", "ë‹¨ë§ê¸°", "ë„¤íŠ¸ì›Œí¬", "ìœ ì‹¬", "esim", "ë¡œë°", "ê²°í•©", "ì¶œì‹œ"]
    return any(inc in t for inc in include)

def get_final_tags(title, db_key, default_tag):
    if not is_telecom_news(title): return None
    t = title.lower().replace(' ', '')
    if db_key == "MNO":
        sa3_kws = ["í†µì‹ 3ì‚¬", "ì´í†µ3ì‚¬", "í†µì‹ ì—…ê³„", "3ì‚¬"]
        skt, kt, lg = "skt" in t or "skí…”ë ˆì½¤" in t, "kt" in t or "ì¼€ì´í‹°" in t, "lgu+" in t or "lgìœ í”ŒëŸ¬ìŠ¤" in t
        if any(x in t for x in sa3_kws) or (skt + kt + lg >= 2): return [{"name": "í†µì‹  3ì‚¬"}]
        elif skt: return [{"name": "SKT"}]
        elif kt: return [{"name": "KT"}]
        elif lg: return [{"name": "LG U+"}]
        return [{"name": default_tag}]
    maps = {
        "SUBSID": {"SKí…”ë§í¬": ["skí…”ë§í¬", "7ëª¨ë°”ì¼"], "KT Mëª¨ë°”ì¼": ["ktmëª¨ë°”ì¼", "ktì— ëª¨ë°”ì¼"], "LGí—¬ë¡œë¹„ì „": ["lgí—¬ë¡œë¹„ì „", "í—¬ë¡œëª¨ë°”ì¼"], "KTìŠ¤ì¹´ì´ë¼ì´í”„": ["ìŠ¤ì¹´ì´ë¼ì´í”„"], "ë¯¸ë””ì–´ë¡œê·¸": ["ë¯¸ë””ì–´ë¡œê·¸", "ìœ ëª¨ë°”ì¼"]},
        "FIN": {"í† ìŠ¤ëª¨ë°”ì¼": ["í† ìŠ¤ëª¨ë°”ì¼", "í† ìŠ¤"], "ìš°ë¦¬ì›ëª¨ë°”ì¼": ["ìš°ë¦¬ì›ëª¨ë°”ì¼", "ìš°ë¦¬ì›"], "KBë¦¬ë¸Œëª¨ë°”ì¼": ["ë¦¬ë¸Œëª¨ë°”ì¼", "ë¦¬ë¸Œm", "kbêµ­ë¯¼"]},
        "SMALL": {"ì•„ì´ì¦ˆëª¨ë°”ì¼": ["ì•„ì´ì¦ˆëª¨ë°”ì¼"], "í”„ë¦¬ëª¨ë°”ì¼": ["í”„ë¦¬í…”ë ˆì½¤", "í”„ë¦¬í‹°"], "ì—ë„¥ìŠ¤í…”ë ˆì½¤": ["ì—ë„¥ìŠ¤í…”ë ˆì½¤", "aëª¨ë°”ì¼"], "ìœ ë‹ˆì»´ì¦ˆ": ["ìœ ë‹ˆì»´ì¦ˆ", "ëª¨ë¹„ìŠ¤íŠ¸"], "ì¸ìŠ¤ì½”ë¹„": ["ì¸ìŠ¤ì½”ë¹„"], "ì„¸ì¢…í…”ë ˆì½¤": ["ì„¸ì¢…í…”ë ˆì½¤", "ìŠ¤ë…¸ìš°ë§¨"], "í°ì‚¬ëŒ": ["í°ì‚¬ëŒ", "ì´ì•¼ê¸°ëª¨ë°”ì¼"]}
    }
    if db_key in maps:
        for name, kws in maps[db_key].items():
            if any(k in t for k in kws): return [{"name": name}]
    return None

def post_notion(db_id, title, link, tags, pub_date):
    target_id = re.sub(r'[^a-fA-F0-9]', '', db_id)
    data = {"parent": {"database_id": target_id}, "properties": {"ì œëª©": {"title": [{"text": {"content": title, "link": {"url": link}}}]}, "ë‚ ì§œ": {"rich_text": [{"text": {"content": pub_date}}]}, "ë§í¬": {"url": link}, "ë¶„ë¥˜": {"multi_select": tags}}}
    res = requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json=data)
    return res.status_code == 200

def collect_news(db_key, configs, default_days=7):
    """í†µí•© ìˆ˜ì§‘ ë¡œì§: ë¶„ë¥˜ë³„ ìµœì†Œ 5ê°œ, ìµœëŒ€ 15ê°œ"""
    db_id = DB_IDS.get(db_key)
    clear_database(db_id)
    
    seen_urls, seen_titles = set(), []
    # 7ì¼(MNOìš©) ë˜ëŠ” 60ì¼(ì•Œëœ°í°ìš©) ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    allowed_dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(default_days + 1)]

    for keywords, _, target_tag in configs:
        tag_count = 0
        print(f"ğŸ“¡ {db_key} - {target_tag} ìˆ˜ì§‘ ì¤‘...")
        
        for sort in ["sim", "date"]:
            if tag_count >= 15: break
            query = " ".join(keywords)
            url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=100&sort={sort}"
            res = requests.get(url, headers={"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET})
            if res.status_code != 200: continue

            for item in res.json().get('items', []):
                if tag_count >= 15: break
                if item['link'] in seen_urls: continue
                
                title = item['title'].replace('<b>','').replace('</b>','').replace('&quot;','"')
                if any(get_similarity(title, st) > 0.45 for st in seen_titles): continue

                tags = get_final_tags(title, db_key, target_tag)
                if tags and tags[0]['name'] == target_tag:
                    p_date = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S +0900').strftime('%Y-%m-%d')
                    
                    # [ë¡œì§] 7ì¼ ì´ë‚´ ê¸°ì‚¬ê±°ë‚˜, í˜¹ì€ ì•„ì§ ìµœì†Œ 5ê°œë¥¼ ëª» ì±„ì› ë‹¤ë©´ ê³¼ê±° ê¸°ì‚¬ë¼ë„ ìˆ˜ì§‘
                    if p_date in allowed_dates or tag_count < 5:
                        if post_notion(db_id, title, item['link'], tags, p_date):
                            seen_urls.add(item['link'])
                            seen_titles.append(title)
                            tag_count += 1
        print(f"âœ… {target_tag}: {tag_count}ê°œ ìˆ˜ì§‘ë¨")

if __name__ == "__main__":
    # 1. MNO (ê¸°ë³¸ 7ì¼ ê¸°ì¤€)
    collect_news("MNO", [
        (["SKí…”ë ˆì½¤", "SKT"], 15, "SKT"), (["KT", "ì¼€ì´í‹°"], 15, "KT"),
        (["LGìœ í”ŒëŸ¬ìŠ¤", "LGU+"], 15, "LG U+"), (["í†µì‹ 3ì‚¬", "í†µì‹ ì—…ê³„"], 15, "í†µì‹  3ì‚¬")
    ], 7)

    # 2. SUBSID (ê¸°ë³¸ 60ì¼ ê¸°ì¤€)
    collect_news("SUBSID", [
        (["SKí…”ë§í¬"], 15, "SKí…”ë§í¬"), (["KTì— ëª¨ë°”ì¼"], 15, "KT Mëª¨ë°”ì¼"),
        (["LGí—¬ë¡œë¹„ì „"], 15, "LGí—¬ë¡œë¹„ì „"), (["ìŠ¤ì¹´ì´ë¼ì´í”„"], 15, "KTìŠ¤ì¹´ì´ë¼ì´í”„"), (["ë¯¸ë””ì–´ë¡œê·¸"], 15, "ë¯¸ë””ì–´ë¡œê·¸")
    ], 60)

    # 3. FIN (ê¸°ë³¸ 30ì¼ ê¸°ì¤€)
    collect_news("FIN", [
        (["í† ìŠ¤ëª¨ë°”ì¼"], 15, "í† ìŠ¤ëª¨ë°”ì¼"), (["ë¦¬ë¸Œëª¨ë°”ì¼"], 15, "KBë¦¬ë¸Œëª¨ë°”ì¼"), (["ìš°ë¦¬ì›ëª¨ë°”ì¼"], 15, "ìš°ë¦¬ì›ëª¨ë°”ì¼")
    ], 30)

    # 4. SMALL (ê¸°ë³¸ 60ì¼ ê¸°ì¤€)
    collect_news("SMALL", [
        (["ì•„ì´ì¦ˆëª¨ë°”ì¼"], 15, "ì•„ì´ì¦ˆëª¨ë°”ì¼"), (["í”„ë¦¬í…”ë ˆì½¤"], 15, "í”„ë¦¬ëª¨ë°”ì¼"), (["ì—ë„¥ìŠ¤í…”ë ˆì½¤"], 15, "ì—ë„¥ìŠ¤í…”ë ˆì½¤"), 
        (["ìœ ë‹ˆì»´ì¦ˆ"], 15, "ìœ ë‹ˆì»´ì¦ˆ"), (["ì¸ìŠ¤ì½”ë¹„"], 15, "ì¸ìŠ¤ì½”ë¹„"), (["ì„¸ì¢…í…”ë ˆì½¤"], 15, "ì„¸ì¢…í…”ë ˆì½¤"), (["í°ì‚¬ëŒ"], 15, "í°ì‚¬ëŒ")
    ], 60)
