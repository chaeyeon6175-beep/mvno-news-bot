import os, requests, re, time
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urlunparse
from difflib import SequenceMatcher

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
NAVER_ID = os.environ.get('NAVER_CLIENT_ID')
NAVER_SECRET = os.environ.get('NAVER_CLIENT_SECRET')
NOTION_TOKEN = os.environ.get('NOTION_TOKEN')
DB_IDS = {
    "MNO": os.environ.get('DB_ID_MNO'),
    "SUBSID": os.environ.get('DB_ID_SUBSID'),
    "FIN": os.environ.get('DB_ID_FIN'),
    "SMALL": os.environ.get('DB_ID_SMALL')
}

HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

def clean_id(raw_id):
    if not raw_id: return ""
    return re.sub(r'[^a-fA-F0-9]', '', raw_id)

def is_similar(title1, title2):
    """ì œëª© ìœ ì‚¬ë„ 70% ì´ìƒì´ê±°ë‚˜ 8ê¸€ì ì—°ì† ì¤‘ë³µ ì‹œ í•„í„°ë§"""
    t1 = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', title1)
    t2 = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', title2)
    ratio = SequenceMatcher(None, t1, t2).ratio()
    match = SequenceMatcher(None, t1, t2).find_longest_match(0, len(t1), 0, len(t2))
    return ratio > 0.7 or match.size >= 8

def validate_link(url):
    """ë§í¬ê°€ ì •ìƒì¸ì§€ í™•ì¸. ë¬¸ì œ ìˆìœ¼ë©´ None ë°˜í™˜ (ì†Œì œëª© ì¶”ì¶œ ê¸°ëŠ¥ ì œê±°)"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        res = requests.get(url, headers=headers, timeout=5, allow_redirects=True)
        if res.status_code != 200 or "ì˜ëª»ëœ ê²½ë¡œ" in res.text or "ì¡´ì¬í•˜ì§€ ì•ŠëŠ”" in res.text:
            return None
        
        soup = BeautifulSoup(res.text, 'html.parser')
        img_tag = soup.find('meta', property='og:image')
        img = img_tag['content'] if img_tag else "https://images.unsplash.com/photo-1504711434969-e33886168f5c?q=80&w=1000"
        return img
    except:
        return None

def post_notion(db_id, title, link, img, tag):
    """ë…¸ì…˜ ì „ì†¡ (ì†Œì œëª© í•­ëª© ì œê±° ë²„ì „)"""
    target_id = clean_id(db_id)
    if not target_id: return False
    data = {
        "parent": {"database_id": target_id},
        "cover": {"type": "external", "external": {"url": img}},
        "properties": {
            "ì œëª©": {"title": [{"text": {"content": title, "link": {"url": link}}}]},
            "ë‚ ì§œ": {"rich_text": [{"text": {"content": datetime.now().strftime('%Y-%m-%d')}}]},
            "ë§í¬": {"url": link},
            "ë¶„ë¥˜": {"multi_select": [{"name": tag}]}
        }
    }
    res = requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json=data)
    return res.status_code == 200

def collect_news(db_key, configs, processed_links, processed_titles):
    db_id = DB_IDS.get(db_key)
    if not db_id: return

    # í•´ë‹¹ DB ê·¸ë£¹ì˜ ëª¨ë“  í‚¤ì›Œë“œë¥¼ í•©ì³ì„œ ë„¤ì´ë²„ ê²€ìƒ‰ (í•œ ë²ˆì— ë§ì´ ê°€ì ¸ì˜´)
    all_keywords = []
    for keywords, _, tag in configs:
        all_keywords.extend(keywords)
    
    search_query = " | ".join([f"\"{k}\"" for k in all_keywords])
    url = f"https://openapi.naver.com/v1/search/news.json?query={search_query}&display=100&sort=sim"
    res = requests.get(url, headers={"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET})
    
    if res.status_code == 200:
        items = res.json().get('items', [])
        
        # ê° íƒœê·¸(ê¸°ì—…)ë³„ë¡œ ìˆ˜ì§‘ëœ ê°œìˆ˜ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ì‚¬ì „
        tag_counts = {cfg[2]: 0 for cfg in configs}
        
        for item in items:
            title = item['title'].replace('<b>','').replace('</b>','').replace('&quot;','"')
            link = item['link'] if 'naver.com' in item['link'] else (item['originallink'] or item['link'])
            
            # 1. ì œëª© ì¤‘ë³µ ê²€ì‚¬
            if any(is_similar(title, prev_title) for prev_title in processed_titles):
                continue

            # 2. ì •ë°€ ë¶„ë¥˜ ë¡œì§: ì œëª©ì— íŠ¹ì • ê¸°ì—… í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            matched_tag = None
            for keywords, limit, tag in configs:
                # í•´ë‹¹ íƒœê·¸ì˜ ìˆ˜ì§‘ ì œí•œëŸ‰ì„ ë„˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
                if tag_counts[tag] >= limit:
                    continue
                
                # ì œëª©ì— í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê²€ì‚¬ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                if any(k.lower() in title.lower() for k in keywords):
                    matched_tag = tag
                    break
            
            # ë§¤ì¹­ëœ íƒœê·¸ê°€ ì—†ìœ¼ë©´(ì œëª©ì— ê¸°ì—…ëª…ì´ ì—†ìœ¼ë©´) ë²„ë¦¼
            if not matched_tag:
                continue

            # 3. ë§í¬ ìœ íš¨ì„± ê²€ì¦
            img = validate_link(link)
            if not img:
                continue
            
            # 4. ë…¸ì…˜ ì „ì†¡
            if post_notion(db_id, title, link, img, matched_tag):
                processed_links.add(link)
                processed_titles.add(title)
                tag_counts[matched_tag] += 1
                print(f"      âœ… [{matched_tag}] ì„±ê³µ: {title[:20]}...")
                time.sleep(0.1)

if __name__ == "__main__":
    links, titles = set(), set()
    
    # [ì„¤ì •] (í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸, ëª©í‘œ ìˆ˜ì§‘ ê°œìˆ˜, íƒœê·¸ëª…)
    mno_configs = [
        (["SKí…”ë ˆì½¤", "SKT"], 3, "SKT"),
        (["KT", "ì¼€ì´í‹°"], 3, "KT"),
        (["LGìœ í”ŒëŸ¬ìŠ¤", "LGU+"], 3, "LG U+"),
        (["í†µì‹  3ì‚¬", "ì´í†µ3ì‚¬"], 2, "í†µì‹  3ì‚¬")
    ]
    subsid_configs = [
        (["SKí…”ë§í¬", "ì„¸ë¸ëª¨ë°”ì¼", "7ëª¨ë°”ì¼"], 3, "SKí…”ë§í¬"),
        (["KT Mëª¨ë°”ì¼", "KTì— ëª¨ë°”ì¼", "ì¼€ì´í‹°ì— ëª¨ë°”ì¼"], 3, "KT Mëª¨ë°”ì¼"),
        (["LGí—¬ë¡œë¹„ì „", "í—¬ë¡œëª¨ë°”ì¼"], 3, "LGí—¬ë¡œë¹„ì „"),
        (["ë¯¸ë””ì–´ë¡œê·¸", "ìœ ëª¨ë°”ì¼", "Uëª¨ë°”ì¼"], 3, "ë¯¸ë””ì–´ë¡œê·¸")
    ]
    fin_configs = [
        (["KBë¦¬ë¸Œëª¨ë°”ì¼", "ë¦¬ë¸Œì— ", "êµ­ë¯¼ì€í–‰ ì•Œëœ°í°"], 3, "KB ë¦¬ë¸Œëª¨ë°”ì¼"),
        (["í† ìŠ¤ëª¨ë°”ì¼", "toss mobile"], 3, "í† ìŠ¤ëª¨ë°”ì¼"),
        (["ìš°ë¦¬ì›ëª¨ë°”ì¼"], 3, "ìš°ë¦¬ì›ëª¨ë°”ì¼")
    ]
    small_configs = [
        (["ì•„ì´ì¦ˆëª¨ë°”ì¼", "eyesmobile"], 2, "ì•„ì´ì¦ˆëª¨ë°”ì¼"),
        (["í”„ë¦¬í…”ë ˆì½¤", "í”„ë¦¬ëª¨ë°”ì¼"], 2, "í”„ë¦¬í…”ë ˆì½¤"),
        (["ì—ë„¥ìŠ¤í…”ë ˆì½¤", "Aëª¨ë°”ì¼"], 2, "ì—ë„¥ìŠ¤í…”ë ˆì½¤"),
        (["ì¸ìŠ¤ëª¨ë°”ì¼"], 2, "ì¸ìŠ¤ëª¨ë°”ì¼")
    ]

    print("ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì •ë°€ ë¶„ë¥˜ ì‹œì‘...")
    collect_news("MNO", mno_configs, links, titles)
    collect_news("SUBSID", subsid_configs, links, titles)
    collect_news("FIN", fin_configs, links, titles)
    collect_news("SMALL", small_configs, links, titles)
