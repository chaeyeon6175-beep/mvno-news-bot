import os, requests, re, time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from difflib import SequenceMatcher

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
NAVER_ID = os.environ.get('NAVER_CLIENT_ID')
NAVER_SECRET = os.environ.get('NAVER_CLIENT_SECRET')
NOTION_TOKEN = os.environ.get('NOTION_TOKEN')
DB_IDS = {
    "MNO": os.environ.get('DB_ID_MNO'),
    "SUBSID": os.environ.get('DB_ID_SUBSID'),
    "FIN": os.environ.get('DB_ID_FIN'),
    "SMALL": os.environ.get('DB_ID_SMALL')
}

HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

def clear_notion_database(db_id):
    """ìˆ˜ì§‘ ì „ ê¸°ì¡´ ë‰´ìŠ¤ ì‚­ì œ(ì•„ì¹´ì´ë¸Œ)"""
    target_id = re.sub(r'[^a-fA-F0-9]', '', db_id or "")
    if not target_id: return
    query_url = f"https://api.notion.com/v1/databases/{target_id}/query"
    try:
        res = requests.post(query_url, headers=HEADERS)
        if res.status_code == 200:
            for page in res.json().get("results", []):
                requests.patch(f"https://api.notion.com/v1/pages/{page['id']}", headers=HEADERS, json={"archived": True})
            print(f"ğŸ—‘ï¸ DB({target_id[:5]}) ì´ˆê¸°í™” ì™„ë£Œ")
    except: pass

def is_similar(title1, title2):
    """8ê¸€ì ì´ìƒ ì—°ì† ì¤‘ë³µ ë˜ëŠ” ìœ ì‚¬ë„ 70% ì´ìƒ ì²´í¬"""
    t1 = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', title1)
    t2 = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', title2)
    ratio = SequenceMatcher(None, t1, t2).ratio()
    match = SequenceMatcher(None, t1, t2).find_longest_match(0, len(t1), 0, len(t2))
    return ratio > 0.7 or match.size >= 8

def validate_link(url):
    """ë§í¬ ìœ íš¨ì„± ë° ì¸ë„¤ì¼(og:image) ì¶”ì¶œ"""
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        img = soup.find('meta', property='og:image')
        return img['content'] if img else "https://images.unsplash.com/photo-1504711434969-e33886168f5c"
    except: return "https://images.unsplash.com/photo-1504711434969-e33886168f5c"

def post_notion(db_id, title, link, img, tag, pub_date):
    """ë…¸ì…˜ì— í˜ì´ì§€ ìƒì„± (pub_date: ë‰´ìŠ¤ ë°°í¬ ë‚ ì§œ)"""
    target_id = re.sub(r'[^a-fA-F0-9]', '', db_id or "")
    data = {
        "parent": {"database_id": target_id},
        "cover": {"type": "external", "external": {"url": img}},
        "properties": {
            "ì œëª©": {"title": [{"text": {"content": title, "link": {"url": link}}}]},
            "ë‚ ì§œ": {"rich_text": [{"text": {"content": pub_date}}]}, # ë‰´ìŠ¤ ë°°í¬ë‚ ì§œ ì¶œë ¥
            "ë§í¬": {"url": link},
            "ë¶„ë¥˜": {"multi_select": [{"name": tag}]}
        }
    }
    return requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json=data).status_code == 200

def classify_mno(title):
    """í†µì‹  3ì‚¬ ë³¸ì²´ ë° ì˜ì¥ ê´€ë ¨ ê¸°ì‚¬ íŒë³„ (MNO ì „ìš©)"""
    t_clean = re.sub(r'\s+', '', title).lower()
    if any(ex in t_clean for ex in ["ktì•ŒíŒŒ", "ktalpha", "ì¼€ì´í‹°ì•ŒíŒŒ"]): return None
    
    mno_all = ["í†µì‹ 3ì‚¬", "ì´í†µ3ì‚¬", "ì´í†µì‚¬", "í†µì‹ ì—…ê³„", "í†µì‹ ì‚¬"]
    skt, kt, lg = ["skí…”ë ˆì½¤", "skt"], ["kt", "ì¼€ì´í‹°"], ["lgìœ í”ŒëŸ¬ìŠ¤", "lgu+", "ì—˜ì§€ìœ í”ŒëŸ¬ìŠ¤", "uí”ŒëŸ¬ìŠ¤", "ìœ í”ŒëŸ¬ìŠ¤"]
    
    if any(k in t_clean for k in mno_all): return "í†µì‹  3ì‚¬"
    
    h_skt, h_kt, h_lg = any(n in t_clean for n in skt), any(n in t_clean for n in kt), any(n in t_clean for n in lg)
    if (h_skt and h_kt) or (h_kt and h_lg) or (h_skt and h_lg): return "í†µì‹  3ì‚¬"
    if h_skt: return "SKT"
    if h_kt: return "KT"
    if h_lg: return "LG U+"
    return None

def collect_news(db_key, configs, processed_titles, days_range):
    """ë‰´ìŠ¤ ìˆ˜ì§‘ í•µì‹¬ ë¡œì§"""
    db_id = DB_IDS.get(db_key)
    if not db_id: return
    allowed_dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_range + 1)]

    for keywords, limit, tag in configs:
        query = " | ".join([f"\"{k}\"" for k in keywords]) if keywords else "í†µì‹ ì‚¬"
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=100&sort=date"
        res = requests.get(url, headers={"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET})
        
        if res.status_code != 200: continue
        
        count = 0
        for item in res.json().get('items', []):
            if count >= limit: break
            
            # [ë‚ ì§œ] ìˆ˜ì§‘ì¼ì´ ì•„ë‹Œ ì‹¤ì œ ë‰´ìŠ¤ ë°°í¬ ë‚ ì§œ ì¶”ì¶œ
            p_date = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S +0900').strftime('%Y-%m-%d')
            if p_date not in allowed_dates: continue
            
            # [ì œëª©] HTML íƒœê·¸ ì œê±° ë° ì œëª© ê¸°ë°˜ í•„í„°ë§
            title = item['title'].replace('<b>','').replace('</b>','').replace('&quot;','"')
            if keywords and not any(k.lower().replace(' ', '') in title.lower().replace(' ', '') for k in keywords):
                continue

            # [ì¤‘ë³µ] 8ê¸€ì ì´ìƒ ì¼ì¹˜ ì‹œ ì œì™¸
            if any(is_similar(title, pt) for pt in processed_titles): continue

            # [MNO ìš°ì„ ] ë³¸ì²´ ê¸°ì‚¬ë¼ë©´ ë‹¤ë¥¸ DB ìˆ˜ì§‘ ì¤‘ì´ë¼ë„ íŒ¨ìŠ¤ (1ë²ˆ DB ì„ ì ìš©)
            mno_check = classify_mno(title)
            if db_key != "MNO" and mno_check is not None: continue

            matched_tag = mno_check if db_key == "MNO" else tag
            if not matched_tag: continue

            img = validate_link(item['link'])
            if post_notion(db_id, title, item['link'], img, matched_tag, p_date):
                processed_titles.add(title)
                count += 1
                print(f"âœ… [{matched_tag}] ({p_date}) ìˆ˜ì§‘ ì„±ê³µ")

if __name__ == "__main__":
    for key in DB_IDS: clear_notion_database(DB_IDS[key])
    titles = set()
    
    # 1. MNO DB (í†µì‹  3ì‚¬ ë³¸ì²´)
    collect_news("MNO", [([], 20, "í†µì‹ ì‚¬")], titles, 1)
    
    # 2. ìíšŒì‚¬ DB
    collect_news("SUBSID", [
        (["SKí…”ë§í¬", "7ëª¨ë°”ì¼"], 3, "SKí…”ë§í¬"),
        (["KT Mëª¨ë°”ì¼", "KTì— ëª¨ë°”ì¼"], 3, "KT Mëª¨ë°”ì¼"),
        (["LGí—¬ë¡œë¹„ì „", "í—¬ë¡œëª¨ë°”ì¼"], 3, "LGí—¬ë¡œë¹„ì „"),
        (["u+ ìœ ëª¨ë°”ì¼", "ìœ í”ŒëŸ¬ìŠ¤ìœ ëª¨ë°”ì¼", "U+ìœ ëª¨ë°”ì¼", "ë¯¸ë””ì–´ë¡œê·¸ ì•Œëœ°í°"], 3, "ë¯¸ë””ì–´ë¡œê·¸")
    ], titles, 1)

    # 3. ê¸ˆìœµê¶Œ DB (KB/ìš°ë¦¬ì€í–‰ í‚¤ì›Œë“œ í™•ì¥)
    collect_news("FIN", [
        (["ë¦¬ë¸Œëª¨ë°”ì¼", "ë¦¬ë¸Œì— ", "êµ­ë¯¼ì€í–‰ ì•Œëœ°í°", "kbêµ­ë¯¼ì€í–‰ ì•Œëœ°í°"], 5, "KB ë¦¬ë¸Œëª¨ë°”ì¼"),
        (["ìš°ë¦¬ì›ëª¨ë°”ì¼", "ìš°ë¦¬ì€í–‰ ì•Œëœ°í°"], 3, "ìš°ë¦¬ì›ëª¨ë°”ì¼"),
        (["í† ìŠ¤ ëª¨ë°”ì¼ ì•Œëœ°í°"], 5, "í† ìŠ¤ëª¨ë°”ì¼")
    ], titles, 60)

    # 4. ì¤‘ì†Œ/ê¸°íƒ€ DB
    collect_news("SMALL", [
        (["ì•„ì´ì¦ˆëª¨ë°”ì¼"], 3, "ì•„ì´ì¦ˆëª¨ë°”ì¼"),
        (["ì¸ìŠ¤ëª¨ë°”ì¼"], 3, "ì¸ìŠ¤ëª¨ë°”ì¼"),
        (["í”„ë¦¬í…”ë ˆì½¤"], 3, "í”„ë¦¬í…”ë ˆì½¤"),
        (["ì—ë„¥ìŠ¤í…”ë ˆì½¤", "Aëª¨ë°”ì¼"], 3, "ì—ë„¥ìŠ¤í…”ë ˆì½¤"),
        (["ìŠ¤ë…¸ìš°ë§¨"], 3, "ìŠ¤ë…¸ìš°ë§¨")
    ], titles, 60)
