import os, requests, re, time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urlunparse
from difflib import SequenceMatcher

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
NAVER_ID = os.environ.get('NAVER_CLIENT_ID')
NAVER_SECRET = os.environ.get('NAVER_CLIENT_SECRET')
NOTION_TOKEN = os.environ.get('NOTION_TOKEN')
DB_IDS = {
    "MNO": os.environ.get('DB_ID_MNO'),
    "SUBSID": os.environ.get('DB_ID_SUBSID'),
    "FIN": os.environ.get('DB_ID_FIN'),
    "SMALL": os.environ.get('DB_ID_SMALL')
}

HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

def clean_id(raw_id):
    if not raw_id: return ""
    return re.sub(r'[^a-fA-F0-9]', '', raw_id)

def is_similar(title1, title2):
    """ì œëª©ì˜ ê³µë°±/íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•˜ê³  80% ì´ìƒ ì¼ì¹˜í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼"""
    t1 = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', title1)
    t2 = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', title2)
    return SequenceMatcher(None, t1, t2).ratio() > 0.8

def validate_link(url):
    """ë§í¬ ìœ íš¨ì„± í™•ì¸ ë° ì´ë¯¸ì§€ ì¶”ì¶œ (ì˜ëª»ëœ ê²½ë¡œ ì°¨ë‹¨)"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        res = requests.get(url, headers=headers, timeout=5)
        if res.status_code != 200 or any(x in res.text for x in ["ì˜ëª»ëœ ê²½ë¡œ", "ì¡´ì¬í•˜ì§€ ì•ŠëŠ”"]):
            return None
        soup = BeautifulSoup(res.text, 'html.parser')
        img_tag = soup.find('meta', property='og:image')
        return img_tag['content'] if img_tag else "https://images.unsplash.com/photo-1504711434969-e33886168f5c?q=80&w=1000"
    except:
        return None

def post_notion(db_id, title, link, img, tag, pub_date):
    target_id = clean_id(db_id)
    if not target_id: return False
    data = {
        "parent": {"database_id": target_id},
        "cover": {"type": "external", "external": {"url": img}},
        "properties": {
            "ì œëª©": {"title": [{"text": {"content": title, "link": {"url": link}}}]},
            "ë‚ ì§œ": {"rich_text": [{"text": {"content": pub_date}}]},
            "ë§í¬": {"url": link},
            "ë¶„ë¥˜": {"multi_select": [{"name": tag}]}
        }
    }
    res = requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json=data)
    return res.status_code == 200

def classify_mno(title):
    t = re.sub(r'\s+', '', title).lower()
    mno_k = ["í†µì‹ 3ì‚¬", "ì´í†µ3ì‚¬", "í†µì‹ ì—…", "í†µì‹ ì‚¬"]
    skt = ["skí…”ë ˆì½¤", "skt"]; kt = ["kt", "ì¼€ì´í‹°"]; lg = ["lgìœ í”ŒëŸ¬ìŠ¤", "lgu+", "ì—˜ì§€ìœ í”ŒëŸ¬ìŠ¤"]
    if any(k in t for k in mno_k): return "í†µì‹  3ì‚¬"
    h_skt = any(n in t for n in skt); h_kt = any(n in t for n in kt); h_lg = any(n in t for n in lg)
    if h_skt and h_kt and h_lg: return "í†µì‹  3ì‚¬"
    if h_skt and not h_kt and not h_lg: return "SKT"
    if h_kt and not h_skt and not h_lg: return "KT"
    if h_lg and not h_skt and not h_kt: return "LG U+"
    return None

def fetch_and_process(db_key, keywords, limit, tag, p_links, p_titles, days_range):
    db_id = DB_IDS.get(db_key)
    if not db_id: return
    
    # ì„¤ì •ëœ ë‚ ì§œ ë²”ìœ„ ë¦¬ìŠ¤íŠ¸
    allowed_dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_range + 1)]
    
    query = " | ".join([f"\"{k}\"" for k in keywords])
    display_count = 100 if days_range > 10 else 50
    url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display={display_count}&sort=sim"
    res = requests.get(url, headers={"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET})
    
    if res.status_code == 200:
        count = 0
        for item in res.json().get('items', []):
            if count >= limit: break
            
            try:
                p_dt = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S +0900')
                p_str = p_dt.strftime('%Y-%m-%d')
            except: continue

            if p_str not in allowed_dates: continue

            title = item['title'].replace('<b>','').replace('</b>','').replace('&quot;','"')
            link = item['link'] if 'naver.com' in item['link'] else (item['originallink'] or item['link'])
            
            # [ì¤‘ë³µ ë°©ì§€ í•µì‹¬] URLë¿ë§Œ ì•„ë‹ˆë¼ ì œëª© ìœ ì‚¬ë„ê¹Œì§€ ì „ì—­ìœ¼ë¡œ ì²´í¬
            if any(is_similar(title, pt) for pt in p_titles): continue
            if not any(k.lower() in title.lower() for k in keywords): continue
            
            final_tag = classify_mno(title) if db_key == "MNO" else tag
            if not final_tag: continue

            img = validate_link(link)
            if not img: continue
            
            if post_notion(db_id, title, link, img, final_tag, p_str):
                p_links.add(link)
                p_titles.add(title) # ì „ì—­ ì œëª© ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ì—¬ ë‹¤ìŒ ê²€ìƒ‰ ì‹œ ë¹„êµ
                print(f"   âœ… [{final_tag}] ({p_str}) {title[:15]}...")
                count += 1
                time.sleep(0.1)

if __name__ == "__main__":
    global_links, global_titles = set(), set()
    
    print("ğŸš€ 1, 2ë²ˆ DB ìˆ˜ì§‘ (5ì¼ ë²”ìœ„)...")
    mno_tasks = [
        (["í†µì‹  3ì‚¬", "ì´í†µ 3ì‚¬"], 5, "í†µì‹  3ì‚¬"),
        (["SKí…”ë ˆì½¤", "SKT"], 5, "SKT"),
        (["KT", "ì¼€ì´í‹°"], 5, "KT"),
        (["LGìœ í”ŒëŸ¬ìŠ¤", "LGU+"], 5, "LG U+")
    ]
    for kws, lim, t in mno_tasks:
        fetch_and_process("MNO", kws, lim, t, global_links, global_titles, 5)

    sub_tasks = [
        (["SKí…”ë§í¬", "ì„¸ë¸ëª¨ë°”ì¼"], 3, "SKí…”ë§í¬"),
        (["KT Mëª¨ë°”ì¼", "KTì— ëª¨ë°”ì¼"], 3, "KT Mëª¨ë°”ì¼"),
        (["LGí—¬ë¡œë¹„ì „", "í—¬ë¡œëª¨ë°”ì¼"], 3, "LGí—¬ë¡œë¹„ì „"),
        (["ë¯¸ë””ì–´ë¡œê·¸", "ìœ ëª¨ë°”ì¼"], 3, "ë¯¸ë””ì–´ë¡œê·¸")
    ]
    for kws, lim, t in sub_tasks:
        fetch_and_process("SUBSID", kws, lim, t, global_links, global_titles, 5)

    print("\nğŸš€ 3, 4ë²ˆ DB ìˆ˜ì§‘ (2ë‹¬ ë²”ìœ„ í™•ëŒ€)...")
    # ê¸ˆìœµ/ì¤‘ì†Œ ì¹´í…Œê³ ë¦¬ëŠ” 60ì¼ ë²”ìœ„ë¡œ ì‹¤í–‰
    fin_tasks = [
        (["KBë¦¬ë¸Œëª¨ë°”ì¼", "ë¦¬ë¸Œì— "], 5, "KB ë¦¬ë¸Œëª¨ë°”ì¼"),
        (["í† ìŠ¤ëª¨ë°”ì¼"], 5, "í† ìŠ¤ëª¨ë°”ì¼"),
        (["ìš°ë¦¬ì›ëª¨ë°”ì¼"], 5, "ìš°ë¦¬ì›ëª¨ë°”ì¼")
    ]
    for kws, lim, t in fin_tasks:
        fetch_and_process("FIN", kws, lim, t, global_links, global_titles, 60)

    small_tasks = [
        (["ì•„ì´ì¦ˆëª¨ë°”ì¼"], 3, "ì•„ì´ì¦ˆëª¨ë°”ì¼"),
        (["í”„ë¦¬í…”ë ˆì½¤", "í”„ë¦¬ëª¨ë°”ì¼"], 3, "í”„ë¦¬í…”ë ˆì½¤"),
        (["ì—ë„¥ìŠ¤í…”ë ˆì½¤", "Aëª¨ë°”ì¼"], 3, "ì—ë„¥ìŠ¤í…”ë ˆì½¤"),
        (["ì¸ìŠ¤ëª¨ë°”ì¼"], 3, "ì¸ìŠ¤ëª¨ë°”ì¼")
    ]
    for kws, lim, t in small_tasks:
        fetch_and_process("SMALL", kws, lim, t, global_links, global_titles, 60)
