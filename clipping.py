import os, requests, re, time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from difflib import SequenceMatcher

# í™˜ê²½ ë³€ìˆ˜ ë° í—¤ë” ì„¤ì •
NAVER_ID, NAVER_SECRET = os.environ.get('NAVER_CLIENT_ID'), os.environ.get('NAVER_CLIENT_SECRET')
NOTION_TOKEN = os.environ.get('NOTION_TOKEN')
DB_IDS = {k: os.environ.get(f'DB_ID_{k}') for k in ["MNO", "SUBSID", "FIN", "SMALL"]}
HEADERS = {"Authorization": f"Bearer {NOTION_TOKEN}", "Content-Type": "application/json", "Notion-Version": "2022-06-28"}

def clear_database(db_id):
    """ìƒˆë¡œìš´ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê¸° ì „, ê¸°ì¡´ì— ìˆëŠ” ëª¨ë“  í˜ì´ì§€ë¥¼ ì‚­ì œ(ì•„ì¹´ì´ë¸Œ)í•©ë‹ˆë‹¤."""
    db_id = re.sub(r'[^a-fA-F0-9]', '', db_id or "")
    if not db_id: return
    
    # 1. DB ë‚´ ëª¨ë“  í˜ì´ì§€ ID ì¡°íšŒ
    url = f"https://api.notion.com/v1/databases/{db_id}/query"
    res = requests.post(url, headers=HEADERS)
    if res.status_code == 200:
        pages = res.json().get("results", [])
        for page in pages:
            # 2. ê° í˜ì´ì§€ ì‚­ì œ(archived=True)
            page_id = page["id"]
            requests.patch(f"https://api.notion.com/v1/pages/{page_id}", headers=HEADERS, json={"archived": True})
        print(f"ğŸ—‘ï¸ DB({db_id[:5]}...) ë‚´ ê¸°ì¡´ ë‰´ìŠ¤ ì‚­ì œ ì™„ë£Œ")

# --- ê¸°ì¡´ ìˆ˜ì§‘ ë° ìœ ì‚¬ë„ ê²€ì‚¬ í•¨ìˆ˜ë“¤ (validate_link, is_similar, post_notion ë“±ì€ ë™ì¼) ---
def is_similar(t1, t2):
    s1, s2 = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', t1), re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', t2)
    return SequenceMatcher(None, s1, s2).ratio() > 0.8

def validate_link(url):
    try:
        h = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=h, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        img = soup.find('meta', property='og:image')
        return img['content'] if img else "https://images.unsplash.com/photo-1518770660439-4636190af475"
    except: return "https://images.unsplash.com/photo-1518770660439-4636190af475"

def post_notion(db_id, title, link, img, tag, pub_date):
    db_id = re.sub(r'[^a-fA-F0-9]', '', db_id or "")
    data = {
        "parent": {"database_id": db_id},
        "cover": {"type": "external", "external": {"url": img}},
        "properties": {
            "ì œëª©": {"title": [{"text": {"content": title, "link": {"url": link}}}]},
            "ë‚ ì§œ": {"rich_text": [{"text": {"content": pub_date}}]},
            "ë§í¬": {"url": link},
            "ë¶„ë¥˜": {"multi_select": [{"name": tag}]}
        }
    }
    return requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json=data).status_code == 200

def fetch_and_process(db_key, configs, p_titles, days):
    db_id = DB_IDS.get(db_key)
    allowed_dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days + 1)]
    
    for keywords, limit, tag in configs:
        query = " | ".join([f"\"{k}\"" for k in keywords])
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=50&sort=sim"
        res = requests.get(url, headers={"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET})
        
        count = 0
        for item in res.json().get('items', []):
            if count >= limit: break
            p_str = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S +0900').strftime('%Y-%m-%d')
            if p_str not in allowed_dates: continue
            title = item['title'].replace('<b>','').replace('</b>','').replace('&quot;','"')
            link = item['link'] if 'naver.com' in item['link'] else item['originallink']
            if any(is_similar(title, pt) for pt in p_titles): continue
            
            img = validate_link(link)
            if post_notion(db_id, title, link, img, tag, p_str):
                p_titles.add(title); count += 1
                print(f"âœ… {tag} ìˆ˜ì§‘: {title[:15]}...")

if __name__ == "__main__":
    # 1. ëª¨ë“  DB ë¹„ìš°ê¸° (ê¸°ì¡´ ë‰´ìŠ¤ ì‚­ì œ)
    print("ğŸ§¹ ê¸°ì¡´ ë‰´ìŠ¤ ì‚­ì œ ì‹œì‘...")
    for key in DB_IDS:
        clear_database(DB_IDS[key])
    
    # 2. ìƒˆë¡œìš´ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘
    print("\nğŸš€ ìƒˆë¡œìš´ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    titles = set()
    # MNO/ìíšŒì‚¬ (5ì¼ ë²”ìœ„)
    fetch_and_process("MNO", [(["SKT", "KT", "LGU+"], 10, "í†µì‹ ì‚¬")], titles, 5)
    # ê¸ˆìœµ/ì¤‘ì†Œ (60ì¼ ë²”ìœ„)
    fetch_and_process("FIN", [(["ë¦¬ë¸Œì— ", "í† ìŠ¤ëª¨ë°”ì¼"], 10, "ê¸ˆìœµê¶Œ")], titles, 60)
    # ... (í•„ìš”í•œ ì¹´í…Œê³ ë¦¬ ì¶”ê°€)
