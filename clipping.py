import os, requests, re, time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urlunparse
from difflib import SequenceMatcher

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
NAVER_ID = os.environ.get('NAVER_CLIENT_ID')
NAVER_SECRET = os.environ.get('NAVER_CLIENT_SECRET')
NOTION_TOKEN = os.environ.get('NOTION_TOKEN')
DB_IDS = {
    "MNO": os.environ.get('DB_ID_MNO'),
    "SUBSID": os.environ.get('DB_ID_SUBSID'),
    "FIN": os.environ.get('DB_ID_FIN'),
    "SMALL": os.environ.get('DB_ID_SMALL')
}

HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

def clean_id(raw_id):
    if not raw_id: return ""
    return re.sub(r'[^a-fA-F0-9]', '', raw_id)

def is_similar(title1, title2):
    """ì œëª© ìœ ì‚¬ë„ ê²€ì‚¬ (ì¤‘ë³µ ë°©ì§€)"""
    t1 = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', title1)
    t2 = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', title2)
    ratio = SequenceMatcher(None, t1, t2).ratio()
    match = SequenceMatcher(None, t1, t2).find_longest_match(0, len(t1), 0, len(t2))
    return ratio > 0.7 or match.size >= 8

def validate_link(url):
    """ë§í¬ê°€ ì •ìƒì¸ì§€ í™•ì¸í•˜ê³  ì´ë¯¸ì§€ ê²½ë¡œ ë°˜í™˜"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        res = requests.get(url, headers=headers, timeout=5)
        if res.status_code != 200 or any(x in res.text for x in ["ì˜ëª»ëœ ê²½ë¡œ", "ì¡´ì¬í•˜ì§€ ì•ŠëŠ”"]):
            return None
        soup = BeautifulSoup(res.text, 'html.parser')
        img_tag = soup.find('meta', property='og:image')
        return img_tag['content'] if img_tag else "https://images.unsplash.com/photo-1504711434969-e33886168f5c?q=80&w=1000"
    except:
        return None

def post_notion(db_id, title, link, img, tag, pub_date):
    """ë…¸ì…˜ ì „ì†¡ (ì†Œì œëª© ì œê±°, ê¸°ì‚¬ ì‘ì„±ì¼ ì ìš©)"""
    target_id = clean_id(db_id)
    if not target_id: return False
    data = {
        "parent": {"database_id": target_id},
        "cover": {"type": "external", "external": {"url": img}},
        "properties": {
            "ì œëª©": {"title": [{"text": {"content": title, "link": {"url": link}}}]},
            "ë‚ ì§œ": {"rich_text": [{"text": {"content": pub_date}}]},
            "ë§í¬": {"url": link},
            "ë¶„ë¥˜": {"multi_select": [{"name": tag}]}
        }
    }
    res = requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json=data)
    return res.status_code == 200

def classify_mno(title):
    """MNO ì •ë°€ ë¶„ë¥˜ (í†µì‹ 3ì‚¬ ìš°ì„ , ë‹¨ì¼ íšŒì‚¬ ì°¨ì„ )"""
    title_clean = re.sub(r'\s+', '', title).lower()
    mno_keywords = ["í†µì‹ 3ì‚¬", "ì´í†µ3ì‚¬", "í†µì‹ ì—…", "í†µì‹ ì‚¬"]
    skt_names = ["skí…”ë ˆì½¤", "skt"]
    kt_names = ["kt", "ì¼€ì´í‹°"]
    lg_names = ["lgìœ í”ŒëŸ¬ìŠ¤", "lgu+", "ì—˜ì§€ìœ í”ŒëŸ¬ìŠ¤"]
    
    # 1. 'í†µì‹  3ì‚¬'ë¡œ ë¶„ë¥˜í•´ì•¼ í•˜ëŠ” ê²½ìš°
    if any(k in title_clean for k in mno_keywords): return "í†µì‹  3ì‚¬"
    
    has_skt = any(n in title_clean for n in skt_names)
    has_kt = any(n in title_clean for n in kt_names)
    has_lg = any(n in title_clean for n in lg_names)
    
    if has_skt and has_kt and has_lg: return "í†µì‹  3ì‚¬"
    
    # 2. ë”± í•œ íšŒì‚¬ë§Œ ì–¸ê¸‰ëœ ê²½ìš°
    found = []
    if has_skt: found.append("SKT")
    if has_kt: found.append("KT")
    if has_lg: found.append("LG U+")
    
    if len(found) == 1: return found[0]
    return None

def collect_news(db_key, configs, processed_links, processed_titles):
    db_id = DB_IDS.get(db_key)
    if not db_id: return

    # ì˜¤ëŠ˜ ê¸°ì¤€ 5ì¼ ì „ê¹Œì§€ì˜ ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    allowed_dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(1, 6)]
    
    # ê²€ìƒ‰ì–´ ìµœì í™” (MNOëŠ” ëŒ€í‘œ ê²€ìƒ‰ì–´ë¡œ ë„“ê²Œ ê²€ìƒ‰)
    search_keywords = []
    if db_key == "MNO":
        search_keywords = ["SKí…”ë ˆì½¤", "KT", "LGìœ í”ŒëŸ¬ìŠ¤", "í†µì‹  3ì‚¬"]
    else:
        for keywords, _, _ in configs: search_keywords.extend(keywords)
    
    query = " | ".join([f"\"{k}\"" for k in search_keywords])
    # ë‚ ì§œ ë²”ìœ„ê°€ ë„“ìœ¼ë¯€ë¡œ ìˆ˜ì§‘ëŸ‰ì„ 100ê°œë¡œ í™•ëŒ€
    url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=100&sort=date"
    res = requests.get(url, headers={"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET})
    
    if res.status_code == 200:
        items = res.json().get('items', [])
        tag_counts = {cfg[2]: 0 for cfg in configs}
        
        for item in items:
            pub_date_dt = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S +0900')
            pub_date_str = pub_date_dt.strftime('%Y-%m-%d')
            
            # 5ì¼ ì „ ~ ì–´ì œ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘
            if pub_date_str not in allowed_dates: continue

            title = item['title'].replace('<b>','').replace('</b>','').replace('&quot;','"')
            link = item['link'] if 'naver.com' in item['link'] else (item['originallink'] or item['link'])
            
            if any(is_similar(title, prev_title) for prev_title in processed_titles): continue

            # ë¶„ë¥˜ ë¡œì§ ì ìš©
            matched_tag = None
            if db_key == "MNO":
                matched_tag = classify_mno(title)
            else:
                for keywords, limit, tag in configs:
                    if tag_counts[tag] >= limit: continue
                    if any(k.lower() in title.lower() for k in keywords):
                        matched_tag = tag; break
            
            if not matched_tag: continue
            
            img = validate_link(link)
            if not img: continue
            
            if post_notion(db_id, title, link, img, matched_tag, pub_date_str):
                processed_links.add(link)
                processed_titles.add(title)
                if matched_tag in tag_counts: tag_counts[matched_tag] += 1
                print(f"      âœ… [{matched_tag}] ({pub_date_str}) ì„±ê³µ: {title[:15]}...")
                time.sleep(0.1)

if __name__ == "__main__":
    links, titles = set(), set()
    # MNO ì„¤ì •
    mno_cfg = [([], 5, "í†µì‹  3ì‚¬"), ([], 5, "SKT"), ([], 5, "KT"), ([], 5, "LG U+")]
    # ìíšŒì‚¬ ì„¤ì •
    sub_cfg = [
        (["SKí…”ë§í¬", "ì„¸ë¸ëª¨ë°”ì¼", "7ëª¨ë°”ì¼"], 4, "SKí…”ë§í¬"),
        (["KT Mëª¨ë°”ì¼", "KTì— ëª¨ë°”ì¼", "ì¼€ì´í‹°ì— ëª¨ë°”ì¼"], 4, "KT Mëª¨ë°”ì¼"),
        (["LGí—¬ë¡œë¹„ì „", "í—¬ë¡œëª¨ë°”ì¼"], 4, "LGí—¬ë¡œë¹„ì „"),
        (["ë¯¸ë””ì–´ë¡œê·¸", "ìœ ëª¨ë°”ì¼", "Uëª¨ë°”ì¼"], 4, "ë¯¸ë””ì–´ë¡œê·¸")
    ]
    # ê¸ˆìœµê¶Œ ë° ì¤‘ì†Œ ì„¤ì • (ë™ì¼ ë°©ì‹)
    fin_cfg = [(["KBë¦¬ë¸Œëª¨ë°”ì¼", "ë¦¬ë¸Œì— "], 3, "KB ë¦¬ë¸Œëª¨ë°”ì¼"), (["í† ìŠ¤ëª¨ë°”ì¼"], 3, "í† ìŠ¤ëª¨ë°”ì¼"), (["ìš°ë¦¬ì›ëª¨ë°”ì¼"], 3, "ìš°ë¦¬ì›ëª¨ë°”ì¼")]
    small_cfg = [(["ì•„ì´ì¦ˆëª¨ë°”ì¼"], 2, "ì•„ì´ì¦ˆëª¨ë°”ì¼"), (["í”„ë¦¬í…”ë ˆì½¤"], 2, "í”„ë¦¬í…”ë ˆì½¤"), (["ì—ë„¥ìŠ¤í…”ë ˆì½¤", "Aëª¨ë°”ì¼"], 2, "ì—ë„¥ìŠ¤í…”ë ˆì½¤"), (["ì¸ìŠ¤ëª¨ë°”ì¼"], 2, "ì¸ìŠ¤ëª¨ë°”ì¼")]

    print("ğŸš€ 5ì¼ì¹˜ ê¸°ì‚¬ ìˆ˜ì§‘ ë° ì •ë°€ ë¶„ë¥˜ ì‹œì‘...")
    collect_news("MNO", mno_cfg, links, titles)
    collect_news("SUBSID", sub_cfg, links, titles)
    collect_news("FIN", fin_cfg, links, titles)
    collect_news("SMALL", small_cfg, links, titles)
