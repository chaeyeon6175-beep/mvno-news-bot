import os, requests, re, time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from difflib import SequenceMatcher

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
NAVER_ID = os.environ.get('NAVER_CLIENT_ID')
NAVER_SECRET = os.environ.get('NAVER_CLIENT_SECRET')
NOTION_TOKEN = os.environ.get('NOTION_TOKEN')

DB_IDS = {
    "MNO": os.environ.get('DB_ID_MNO'),
    "SUBSID": os.environ.get('DB_ID_SUBSID'),
    "FIN": os.environ.get('DB_ID_FIN'),
    "SMALL": os.environ.get('DB_ID_SMALL')
}

HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

def clear_notion_database(db_id):
    if not db_id: return
    target_id = re.sub(r'[^a-fA-F0-9]', '', db_id)
    try:
        res = requests.post(f"https://api.notion.com/v1/databases/{target_id}/query", headers=HEADERS)
        if res.status_code == 200:
            pages = res.json().get("results", [])
            for page in pages:
                requests.patch(f"https://api.notion.com/v1/pages/{page['id']}", headers=HEADERS, json={"archived": True})
            print(f"ğŸ—‘ï¸ DB ì´ˆê¸°í™” ì™„ë£Œ: {target_id[:5]}")
    except: pass

def is_duplicate_by_8_chars(new_title, processed_titles):
    t1 = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', new_title)
    for prev_title in processed_titles:
        t2 = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', prev_title)
        if SequenceMatcher(None, t1, t2).find_longest_match(0, len(t1), 0, len(t2)).size >= 8:
            return True
    return False

def post_notion(db_id, title, link, tag, pub_date, desc=""):
    """
    [ì£¼ì˜] ì•„ë˜ propertiesì˜ 'ì œëª©', 'ë‚ ì§œ', 'ë§í¬', 'ë¶„ë¥˜'ëŠ” 
    ë…¸ì…˜ DBì˜ ì—´(ì†ì„±) ì´ë¦„ê³¼ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    if not db_id: return False
    target_id = re.sub(r'[^a-fA-F0-9]', '', db_id)
    
    img = "https://images.unsplash.com/photo-1504711434969-e33886168f5c"
    try:
        r = requests.get(link, timeout=5, headers={'User-Agent':'Mozilla/5.0'})
        soup = BeautifulSoup(r.text, 'html.parser')
        og_img = soup.find('meta', property='og:image')
        if og_img: img = og_img['content']
    except: pass

    data = {
        "parent": {"database_id": target_id},
        "cover": {"type": "external", "external": {"url": img}},
        "properties": {
            "ì œëª©": {"title": [{"text": {"content": title}}]}, # ë…¸ì…˜ DB ì—´ ì´ë¦„ í™•ì¸ í•„ìˆ˜
            "ë‚ ì§œ": {"rich_text": [{"text": {"content": pub_date}}]},
            "ë§í¬": {"url": link},
            "ë¶„ë¥˜": {"multi_select": [{"name": tag}]}
        }
    }
    
    # SKí…”ë§í¬ ê¸°ì‚¬ì—ë§Œ ë³¸ë¬¸ ë‚´ìš© ì¶”ê°€
    if desc and "SKí…”ë§í¬" in tag:
        data["children"] = [{"object":"block","type":"paragraph","paragraph":{"rich_text":[{"type":"text","text":{"content":desc[:1000]}}]}}]
    
    res = requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json=data)
    return res.status_code == 200

def classify_mno(title):
    t = re.sub(r'\s+', '', title).lower()
    # ì œì™¸ í‚¤ì›Œë“œ
    if any(ex in t for ex in ["skì‰´ë”ìŠ¤", "ì§€ë‹ˆë®¤ì§", "ktì•ŒíŒŒ"]): return None
    # ìíšŒì‚¬ í‚¤ì›Œë“œ (MNO ìˆ˜ì§‘ì—ì„œ ì œì™¸í•˜ì—¬ SUBSIDë¡œ ìœ ë„)
    if any(sub in t for sub in ["skí…”ë§í¬", "7ëª¨ë°”ì¼", "ktmëª¨ë°”ì¼", "í—¬ë¡œëª¨ë°”ì¼"]): return None

    skt = any(x in t for x in ["skt", "skí…”ë ˆì½¤"])
    kt = any(x in t for x in ["kt", "ì¼€ì´í‹°"])
    lg = any(x in t for x in ["lgu+", "lgìœ í”ŒëŸ¬ìŠ¤"])
    
    if skt and not (kt or lg): return "SKT"
    if kt and not (skt or lg): return "KT"
    if lg and not (skt or kt): return "LG U+"
    if (sum([skt, kt, lg]) >= 2) or any(k in t for k in ["í†µì‹ ì‚¬", "ì´í†µì‚¬"]): return "í†µì‹  3ì‚¬"
    return None

def collect(db_key, configs, processed_titles, days):
    db_id = DB_IDS.get(db_key)
    if not db_id: return

    allowed_dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days + 1)]
    
    for keywords, limit, tag in configs:
        query = " | ".join([f"\"{k}\"" for k in keywords])
        query += " -\"SKì‰´ë”ìŠ¤\" -\"ì§€ë‹ˆë®¤ì§\""
        
        # ìµœì‹ ìˆœ ì •ë ¬ë¡œ ê³ ì •í•˜ì—¬ ë‰´ìŠ¤ ì‹ ë¢°ë„ í™•ë³´
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=100&sort=date"
        res = requests.get(url, headers={"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET})
        
        if res.status_code != 200: continue

        count = 0
        for item in res.json().get('items', []):
            p_date = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S +0900').strftime('%Y-%m-%d')
            title = item['title'].replace('<b>','').replace('</b>','').replace('&quot;','"')
            link = item['link']
            
            if is_duplicate_by_8_chars(title, processed_titles): continue

            # ë¶„ë¥˜ ë¡œì§
            if db_key == "MNO":
                mno_tag = classify_mno(title)
                if mno_tag != tag: continue
                final_tag = mno_tag
            else:
                if not any(k.lower() in title.lower() for k in keywords): continue
                final_tag = tag

            # ë‚ ì§œ í•„í„° (ê¸ˆìœµ/ì¤‘ì†ŒëŠ” ìµœì†Œ 2ê°œ ê°•ì œ ìˆ˜ì§‘)
            if (p_date in allowed_dates) or (db_key in ["FIN", "SMALL"] and count < 2):
                if post_notion(db_id, title, link, final_tag, p_date, item['description']):
                    processed_titles.add(title)
                    count += 1
                    print(f"âœ… [{final_tag}] ìˆ˜ì§‘ ì„±ê³µ: {title[:20]}...")
            if count >= limit: break

if __name__ == "__main__":
    for k in DB_IDS: clear_notion_database(DB_IDS[k])
    titles = set()
    
    # 1. ìíšŒì‚¬ (60ì¼) - SKí…”ë§í¬ ìµœìš°ì„ 
    collect("SUBSID", [(["SKí…”ë§í¬", "7ëª¨ë°”ì¼"], 10, "SKí…”ë§í¬"), (["KT Mëª¨ë°”ì¼"], 5, "KT Mëª¨ë°”ì¼"), (["í—¬ë¡œëª¨ë°”ì¼"], 5, "LGí—¬ë¡œë¹„ì „")], titles, 60)

    # 2. MNO (7ì¼ë¡œ í™•ëŒ€í•˜ì—¬ SKT 10ê°œ í™•ë³´)
    collect("MNO", [(["SKí…”ë ˆì½¤", "SKT"], 15, "SKT"), (["KT", "ì¼€ì´í‹°"], 10, "KT"), (["LGìœ í”ŒëŸ¬ìŠ¤"], 10, "LG U+"), (["í†µì‹ ì‚¬"], 10, "í†µì‹  3ì‚¬")], titles, 7)

    # 3. ê¸ˆìœµ/ì¤‘ì†Œ (60ì¼)
    collect("FIN", [(["ë¦¬ë¸Œëª¨ë°”ì¼", "í† ìŠ¤ëª¨ë°”ì¼"], 5, "ê¸ˆìœµê¶Œ ì•Œëœ°í°")], titles, 60)
    collect("SMALL", [(["ì•Œëœ°í°"], 5, "ì¤‘ì†Œ ì•Œëœ°í°")], titles, 60)
