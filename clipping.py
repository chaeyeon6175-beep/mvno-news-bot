import os, requests, re, time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from difflib import SequenceMatcher

# 1. ì „ì—­ ë³€ìˆ˜ ì„¤ì • (ìµœìƒë‹¨ì— ë°°ì¹˜í•˜ì—¬ ì–´ë””ì„œë“  ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ í•¨)
NAVER_ID = os.environ.get('NAVER_CLIENT_ID')
NAVER_SECRET = os.environ.get('NAVER_CLIENT_SECRET')
NOTION_TOKEN = os.environ.get('NOTION_TOKEN')
DB_IDS = {
    "MNO": os.environ.get('DB_ID_MNO'),
    "SUBSID": os.environ.get('DB_ID_SUBSID'),
    "FIN": os.environ.get('DB_ID_FIN'),
    "SMALL": os.environ.get('DB_ID_SMALL')
}

HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

# --- ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---

def clear_notion_database(db_id):
    if not db_id: return
    target_id = re.sub(r'[^a-fA-F0-9]', '', db_id)
    try:
        res = requests.post(f"https://api.notion.com/v1/databases/{target_id}/query", headers=HEADERS)
        if res.status_code == 200:
            for page in res.json().get("results", []):
                requests.patch(f"https://api.notion.com/v1/pages/{page['id']}", headers=HEADERS, json={"archived": True})
            print(f"ğŸ—‘ï¸ DB({target_id[:5]}...) ì´ˆê¸°í™” ì™„ë£Œ")
    except: pass

def is_duplicate_by_8_chars(new_title, processed_titles):
    t1 = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', new_title)
    for prev_title in processed_titles:
        t2 = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', prev_title)
        match = SequenceMatcher(None, t1, t2).find_longest_match(0, len(t1), 0, len(t2))
        if match.size >= 8: return True
    return False

def validate_link(url):
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        img = soup.find('meta', property='og:image')
        return img['content'] if img else "https://images.unsplash.com/photo-1504711434969-e33886168f5c"
    except: return "https://images.unsplash.com/photo-1504711434969-e33886168f5c"

def post_notion(db_id, title, link, img, tag, pub_date, content=""):
    if not db_id: return False
    target_id = re.sub(r'[^a-fA-F0-9]', '', db_id)
    data = {
        "parent": {"database_id": target_id},
        "cover": {"type": "external", "external": {"url": img}},
        "properties": {
            "ì œëª©": {"title": [{"text": {"content": title, "link": {"url": link}}}]},
            "ë‚ ì§œ": {"rich_text": [{"text": {"content": pub_date}}]},
            "ë§í¬": {"url": link},
            "ë¶„ë¥˜": {"multi_select": [{"name": tag}]}
        }
    }
    if content:
        data["children"] = [{"object": "block", "type": "paragraph", "paragraph": {"rich_text": [{"type": "text", "text": {"content": content}}]}}]
    return requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json=data).status_code == 200

def classify_mno_precision(title):
    t_clean = re.sub(r'\s+', '', title).lower()
    if any(ex in t_clean for ex in ["skì‰´ë”ìŠ¤", "ì§€ë‹ˆë®¤ì§", "ktì•ŒíŒŒ", "ktalpha"]): return None
    if any(sub in t_clean for sub in ["skí…”ë§í¬", "7ëª¨ë°”ì¼", "ì„¸ë¸ëª¨ë°”ì¼", "ktmëª¨ë°”ì¼", "í—¬ë¡œëª¨ë°”ì¼", "ìœ ëª¨ë°”ì¼"]): return None

    skt_k, kt_k, lg_k = ["skí…”ë ˆì½¤", "skt"], ["kt", "ì¼€ì´í‹°"], ["lgìœ í”ŒëŸ¬ìŠ¤", "lgu+", "ì—˜ì§€ìœ í”ŒëŸ¬ìŠ¤", "uí”ŒëŸ¬ìŠ¤", "ìœ í”ŒëŸ¬ìŠ¤"]
    h_skt, h_kt, h_lg = any(n in t_clean for n in skt_k), any(n in t_clean for n in kt_k), any(n in t_clean for n in lg_k)
    
    if h_skt and not (h_kt or h_lg): return "SKT"
    if h_kt and not (h_skt or h_lg): return "KT"
    if h_lg and not (h_skt or h_kt): return "LG U+"
    if (sum([h_skt, h_kt, h_lg]) >= 2) or any(k in t_clean for k in ["í†µì‹ 3ì‚¬", "ì´í†µ3ì‚¬", "ì´í†µì‚¬", "í†µì‹ ì‚¬"]): return "í†µì‹  3ì‚¬"
    return None

# --- ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ ---

def collect_news(db_key, configs, processed_titles, days_range):
    db_id = DB_IDS.get(db_key)
    if not db_id: return
    allowed_dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days_range + 1)]

    for keywords, limit, tag in configs:
        query = " | ".join([f"\"{k}\"" for k in keywords]) if keywords else "ì•Œëœ°í°"
        query += " -\"SKì‰´ë”ìŠ¤\" -\"ì§€ë‹ˆë®¤ì§\""
        
        items = []
        for sort_opt in ["date", "sim"]:
            res = requests.get(f"https://openapi.naver.com/v1/search/news.json?query={query}&display=100&sort={sort_opt}", 
                               headers={"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET})
            if res.status_code == 200:
                items = res.json().get('items', [])
                if items: break

        count = 0
        for item in items:
            p_date = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S +0900').strftime('%Y-%m-%d')
            title = item['title'].replace('<b>','').replace('</b>','').replace('&quot;','"')
            desc = item['description'].replace('<b>','').replace('</b>','').replace('&quot;','"')
            t_compare = title.lower().replace(' ', '')

            if is_duplicate_by_8_chars(title, processed_titles): continue

            if db_key == "MNO":
                mno_check = classify_mno_precision(title)
                if mno_check != tag: continue
                final_tag, content_to_send = mno_check, ""
            else:
                if not any(k.lower().replace(' ', '') in t_compare for k in keywords): continue
                if classify_mno_precision(title) is not None: continue
                final_tag, content_to_send = tag, (desc if "SKí…”ë§í¬" in tag else "")

            # ê¸°ê°„ ì¡°ê±´ + ê¸ˆìœµ/ì¤‘ì†Œ ìµœì†Œ 2ê°œ ë³´ì¥
            if (db_key in ["FIN", "SMALL"] and count < 2) or (p_date in allowed_dates):
                if post_notion(db_id, title, item['link'], validate_link(item['link']), final_tag, p_date, content_to_send):
                    processed_titles.add(title)
                    count += 1
                    print(f"âœ… [{final_tag}] {p_date} ìˆ˜ì§‘ ì„±ê³µ")
            if count >= limit: break

# --- ì‹¤í–‰ ë©”ì¸ ë¡œì§ ---

if __name__ == "__main__":
    # 1. ì´ˆê¸°í™” (DB_IDSê°€ ìƒë‹¨ì— ì •ì˜ë˜ì–´ ìˆì–´ ì´ì œ ì—ëŸ¬ê°€ ë‚˜ì§€ ì•ŠìŒ)
    for k in DB_IDS: 
        if DB_IDS[k]: clear_notion_database(DB_IDS[k])
    
    titles = set()
    
    # 2. ìíšŒì‚¬ (60ì¼ ë²”ìœ„, SKí…”ë§í¬ ìš°ì„  ìˆ˜ì§‘)
    collect_news("SUBSID", [
        (["SKí…”ë§í¬", "7ëª¨ë°”ì¼", "ì„¸ë¸ëª¨ë°”ì¼", "ì—ìŠ¤ì¼€ì´í…”ë§í¬"], 10, "SKí…”ë§í¬"),
        (["KT Mëª¨ë°”ì¼", "KTì— ëª¨ë°”ì¼"], 5, "KT Mëª¨ë°”ì¼"),
        (["LGí—¬ë¡œë¹„ì „", "í—¬ë¡œëª¨ë°”ì¼"], 5, "LGí—¬ë¡œë¹„ì „"),
        (["ìœ ëª¨ë°”ì¼", "U+ìœ ëª¨ë°”ì¼"], 5, "ë¯¸ë””ì–´ë¡œê·¸")
    ], titles, 60)
    
    # 3. MNO (3ì¼ ë²”ìœ„ë¡œ ì†Œí­ í™•ëŒ€í•˜ì—¬ SKT 10ê°œ í™•ë³´ ë³´ì¥)
    collect_news("MNO", [
        (["SKí…”ë ˆì½¤", "SKT"], 20, "SKT"),
        (["KT", "ì¼€ì´í‹°"], 10, "KT"),
        (["LGìœ í”ŒëŸ¬ìŠ¤", "LGU+"], 10, "LG U+"),
        (["í†µì‹ ì‚¬", "ì´í†µì‚¬"], 5, "í†µì‹  3ì‚¬")
    ], titles, 3)
    
    # 4. ê¸ˆìœµ/ì¤‘ì†Œ (60ì¼ ë²”ìœ„, ê¸°ì‚¬ ì—†ì„ ì‹œ ìµœì†Œ 2ê°œ ì¶œë ¥)
    collect_news("FIN", [
        (["ë¦¬ë¸Œëª¨ë°”ì¼", "ë¦¬ë¸Œì— "], 5, "KB ë¦¬ë¸Œëª¨ë°”ì¼"),
        (["ìš°ë¦¬ì›ëª¨ë°”ì¼"], 5, "ìš°ë¦¬ì›ëª¨ë°”ì¼"),
        (["í† ìŠ¤ëª¨ë°”ì¼"], 5, "í† ìŠ¤ëª¨ë°”ì¼")
    ], titles, 60)
    
    collect_news("SMALL", [
        (["ì•„ì´ì¦ˆëª¨ë°”ì¼", "ì¸ìŠ¤ëª¨ë°”ì¼", "í”„ë¦¬í…”ë ˆì½¤", "ì—ë„¥ìŠ¤í…”ë ˆì½¤", "Aëª¨ë°”ì¼"], 5, "ì¤‘ì†Œ ì•Œëœ°í°")
    ], titles, 60)
